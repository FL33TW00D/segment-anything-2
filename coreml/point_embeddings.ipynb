{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1b5ef05-3838-4131-93e6-6fbfb509ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbf2660-c4cc-4203-aabb-a9a662adbf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-small\", device=device)\n",
    "\n",
    "image = Image.open(\"../notebooks/images/truck.jpg\")\n",
    "image = np.array(image.convert(\"RGB\"))\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a146511-3f6f-47c1-9d03-cc9ec5a0f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = np.array([425, 600, 700, 875])\n",
    "input_point = np.array([[575, 750]])\n",
    "input_label = np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20edc654-a155-4129-809a-84658f20e8d8",
   "metadata": {},
   "source": [
    "Prepare coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d5e991-49f3-4a73-953e-150107c8d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_points(point_coords=None, point_labels=None, boxes=None):\n",
    "    if point_coords is not None:\n",
    "        concat_points = (point_coords, point_labels)\n",
    "    else:\n",
    "        concat_points = None\n",
    "    \n",
    "    # Embed prompts\n",
    "    if boxes is not None:\n",
    "        box_coords = boxes.reshape(-1, 2, 2)\n",
    "        box_labels = torch.tensor([[2, 3]], dtype=torch.int, device=boxes.device)\n",
    "        box_labels = box_labels.repeat(boxes.size(0), 1)\n",
    "        # we merge \"boxes\" and \"points\" into a single \"concat_points\" input (where\n",
    "        # boxes are added at the beginning) to sam_prompt_encoder\n",
    "        if concat_points is not None:\n",
    "            concat_coords = torch.cat([box_coords, concat_points[0]], dim=1)\n",
    "            concat_labels = torch.cat([box_labels, concat_points[1]], dim=1)\n",
    "            concat_points = (concat_coords, concat_labels)\n",
    "        else:\n",
    "            concat_points = (box_coords, box_labels)\n",
    "    return concat_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e808401-28e5-41aa-8e40-f7d322d96a7e",
   "metadata": {},
   "source": [
    "This is the actual encoder being invoked (`boxes` is always `None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb786d53-ca52-41ed-96b6-7ce00064f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_embeddings, dense_embeddings = self.model.sam_prompt_encoder(\n",
    "#     points=concat_points,\n",
    "#     boxes=None,\n",
    "#     masks=mask_input,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e81f13-3d3b-49f5-b539-8cae0c3a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n",
    "    input_point, input_label, input_box, None, True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07fe10-0721-4f12-93a2-5a10869df567",
   "metadata": {},
   "source": [
    "Compare preparation: points, box, both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e4a591-7dd8-49ec-b70e-c16476133885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[327.1111, 640.0000]]]), tensor([[0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_only = prepare_points(unnorm_coords, labels)\n",
    "points_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f711ea-2a45-430c-9bec-b239e0a947bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[241.7778, 512.0000],\n",
       "          [398.2222, 746.6667]]]),\n",
       " tensor([[2, 3]], dtype=torch.int32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_only = prepare_points(boxes=unnorm_box)\n",
    "box_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e44920d-cf6f-42c0-b73b-dec65863be7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[241.7778, 512.0000],\n",
       "          [398.2222, 746.6667],\n",
       "          [327.1111, 640.0000]]]),\n",
       " tensor([[2, 3, 0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_and_box = prepare_points(unnorm_coords, labels, unnorm_box)\n",
    "points_and_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6251c2-5432-4b06-8ccd-cbd80eb2ea8c",
   "metadata": {},
   "source": [
    "Compare embeddings and save for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5970d3c-01c3-4275-8e93-efe888513dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_to_embeddings(prepared_points):\n",
    "    with torch.inference_mode():\n",
    "        embeddings = predictor.model.sam_prompt_encoder(prepared_points, boxes=None, masks=None)\n",
    "        embeddings_with_points_only = predictor.model.sam_prompt_encoder.points_only(prepared_points)\n",
    "    for a, b in zip(embeddings, embeddings_with_points_only):\n",
    "        assert torch.allclose(a, b)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b99dc52-80a9-4e5f-9b09-70687076e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_only_embeddings = points_to_embeddings(points_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69a94bf3-d512-4ced-8575-f503b7d93dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 2, 256]), torch.Size([1, 256, 64, 64])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in points_only_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92c57f50-a0c6-49a7-bd9a-155ffd889c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_only_embeddings = points_to_embeddings(box_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d787b7ed-8706-4aeb-9d23-c58245c3a5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 3, 256]), torch.Size([1, 256, 64, 64])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in box_only_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3912e4ea-778e-4094-93ee-07c25279126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_and_box_embeddings = points_to_embeddings(points_and_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74e4831d-6d11-4f82-ace5-262eb0cb8f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 4, 256]), torch.Size([1, 256, 64, 64])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in points_and_box_embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d738b-0af9-4723-a387-16ce13006ea2",
   "metadata": {},
   "source": [
    "## Compare with Core ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "951824be-f5ea-4bf8-9187-b01e2aa3bbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:coremltools:scikit-learn version 1.4.1.post1 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "WARNING:coremltools:Torch version 2.4.0 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7584ef8-6f7a-4936-ba0f-4aa5979e7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_prompt_encoder = ct.models.MLModel(\"sam2_small_prompt_encoder.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8788f8bf-9896-403d-8a05-4fa4b5465750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(points, labels):\n",
    "    output = coreml_prompt_encoder.predict({\"points\": points, \"labels\": labels})\n",
    "    return output[\"sparse_embeddings\"], output[\"dense_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d88291f2-538c-43be-9e7f-11565d560a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 256), (1, 256, 64, 64)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_only_coreml_embeddings = encode_prompt(points_only[0], points_only[1])\n",
    "[x.shape for x in points_only_coreml_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "638c31b9-4470-422b-87b7-0e83361b520f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(points_only_coreml_embeddings[1] - points_only_embeddings[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af942e04-c830-4f70-87df-c1080bab5fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4, 256), (1, 256, 64, 64)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_and_box_coreml_embeddings = encode_prompt(points_and_box[0], points_and_box[1])\n",
    "[x.shape for x in points_and_box_coreml_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62b4c9db-2d7b-407d-9b7c-50cfe2668a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(points_and_box_coreml_embeddings[1] - points_and_box_embeddings[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e073df-b423-4cee-8f36-2ab406215af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
